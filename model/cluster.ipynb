{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\"../\"))\n",
    "import pandas\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import t4c22\n",
    "from t4c22.misc.t4c22_logging import t4c_apply_basic_logging_config\n",
    "from t4c22.t4c22_config import load_basedir\n",
    "from t4c22.misc.parquet_helpers import load_df_from_parquet\n",
    "import tqdm\n",
    "t4c_apply_basic_logging_config(loglevel=\"DEBUG\")\n",
    "# Load BASEDIR from file, change to your data root.\n",
    "BASEDIR = load_basedir(fn=\"t4c22_config.json\", pkg=t4c22)\n",
    "USE_ETA_BASELINE_SNAPSHOTS = False\n",
    "EXPERIMENT_NAME = 'cc_10_clusters'\n",
    "NUM_VOLUME_CLUSTERS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_input(city):\n",
    "    train_input_frames = []\n",
    "    for train_input_file in sorted((BASEDIR / 'train' / city / 'input').glob('counters_*.parquet')):\n",
    "        train_input_frames.append(pandas.read_parquet(train_input_file))\n",
    "    print(f'Read {len(train_input_frames)} training input files for {city}')\n",
    "    train_input = pandas.concat(train_input_frames)\n",
    "\n",
    "    train_input['vol'] = np.array(train_input['volumes_1h'].to_numpy().tolist()).sum(axis=1)\n",
    "    return train_input\n",
    "\n",
    "def get_cluster_ids(volume_clusters, df, group_fields):\n",
    "    df_groups = df.groupby(group_fields).sum()[['vol']]\n",
    "    df_groups = df_groups.reset_index()\n",
    "    df_groups['cluster'] = [get_cluster_id(volume_clusters, vol) for vol in df_groups['vol']]\n",
    "    return df_groups\n",
    "\n",
    "\n",
    "def load_tests(city):\n",
    "    test_input = pandas.read_parquet(BASEDIR / 'test' / city / 'input' / 'counters_test.parquet')\n",
    "    test_input['vol'] = np.array(test_input['volumes_1h'].to_numpy().tolist()).sum(axis=1)\n",
    "    return test_input\n",
    "def get_cluster_id(volume_clusters, vol):\n",
    "    for id, lower_bound, upper_bound in volume_clusters:\n",
    "        if vol >= lower_bound and vol < upper_bound:\n",
    "            return id\n",
    "    return -1\n",
    "\n",
    "\n",
    "def get_cluster_ids(volume_clusters, df, group_fields):\n",
    "    df_groups = df.groupby(group_fields).sum()[['vol']]\n",
    "    df_groups = df_groups.reset_index()\n",
    "    df_groups['cluster'] = [get_cluster_id(volume_clusters, vol) for vol in df_groups['vol']]\n",
    "    return df_groups\n",
    "\n",
    "STATIC_VOLUME_CLUSTERS = {\n",
    "    1: {\n",
    "        'london': [(0, -46599.0, 7806420.0)],\n",
    "        'madrid': [(0, 263688.0, 12361861.0)],\n",
    "        'melbourne': [(0, 35418.79999999999, 6885818.7)]\n",
    "    },\n",
    "    10: {\n",
    "        'london': [\n",
    "            (0, -46599.0, 932240.2999999999),\n",
    "            (1, 932240.2999999999, 1554726.4),\n",
    "            (2, 1554726.4, 2588921.9000000004),\n",
    "            (3, 2588921.9000000004, 3921954.0),\n",
    "            (4, 3921954.0, 4863117.5),\n",
    "            (5, 4863117.5, 5771968.0),\n",
    "            (6, 5771968.0, 6005322.7),\n",
    "            (7, 6005322.7, 6232288.4),\n",
    "            (8, 6232288.4, 6493853.5),\n",
    "            (9, 6493853.5, 7806420.0)\n",
    "        ],\n",
    "        'madrid': [\n",
    "            (0, 263688.0, 923316.4),\n",
    "            (1, 923316.4, 1911206.0),\n",
    "            (2, 1911206.0, 3344747.900000001),\n",
    "            (3, 3344747.900000001, 4972842.2),\n",
    "            (4, 4972842.2, 6422392.0),\n",
    "            (5, 6422392.0, 7669201.8),\n",
    "            (6, 7669201.8, 8975374.4),\n",
    "            (7, 8975374.4, 9762011.8),\n",
    "            (8, 9762011.8, 10563257.9),\n",
    "            (9, 10563257.9, 12361861.0)\n",
    "        ],\n",
    "        'melbourne': [\n",
    "            (0, 35418.79999999999, 344195.01666666666),\n",
    "            (1, 344195.01666666666, 588418.9400000001),\n",
    "            (2, 588418.9400000001, 1072442.9133333336),\n",
    "            (3, 1072442.9133333336, 1817113.62),\n",
    "            (4, 1817113.62, 2663038.05),\n",
    "            (5, 2663038.05, 3292800.466666667),\n",
    "            (6, 3292800.466666667, 3936777.729999998),\n",
    "            (7, 3936777.729999998, 4662487.213333336),\n",
    "            (8, 4662487.213333336, 5325993.843333334),\n",
    "            (9, 5325993.843333334, 6885818.7)\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "if NUM_VOLUME_CLUSTERS in STATIC_VOLUME_CLUSTERS:\n",
    "    city_volume_clusters = STATIC_VOLUME_CLUSTERS[NUM_VOLUME_CLUSTERS]\n",
    "else:\n",
    "    print('Computing volume clusters:')\n",
    "    city_volume_clusters = {\n",
    "        'london': find_volume_clusters('london'),\n",
    "        'madrid': find_volume_clusters('madrid'),\n",
    "        'melbourne': find_volume_clusters('melbourne')\n",
    "    }\n",
    "print(city_volume_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_counts_cc(city):\n",
    "    snapshot_file = BASEDIR / 'snapshots_cc' / f'cc_volume_cluster_baseline_{EXPERIMENT_NAME}_{city}.parquet'\n",
    "    if USE_ETA_BASELINE_SNAPSHOTS:\n",
    "        counts_cc_df = pandas.read_parquet(snapshot_file)\n",
    "    else:\n",
    "        train_inputs_df = load_train_input(city)\n",
    "        print(f'Inputs: {len(train_inputs_df)}')\n",
    "        cluster_dates_df = get_cluster_ids(city_volume_clusters[city], train_inputs_df, ['day', 't'])\n",
    "        print(f'Inputs grouped: {len(cluster_dates_df)}')\n",
    "        \n",
    "        train_labels_df = load_train_labels(city)\n",
    "        print(f'Labels: {len(train_labels_df)}')\n",
    "        train_labels_df = train_labels_df.merge(cluster_dates_df, on=['day', 't'])\n",
    "        print(f'Labels merged: {len(train_labels_df)}')\n",
    "        # print(f'Unique supersegments: {len(train_labels_df[[\"u\",\"v\"]].unique())}')\n",
    "        # train_labels_df = train_labels_df[['u','v' ,'cluster', 'cc']]\n",
    "        df_grouped = train_labels_df.groupby(['u','v' ,'cluster','cc']).count().reset_index()\n",
    "        # counts_cc_df = train_labels_df.groupby(['u','v', 'cluster'])\n",
    "        assert df_grouped[\"vol\"].sum() == len(train_labels_df), (df_grouped[\"cluster\"].sum(), len(train_labels_df))\n",
    "        counts_cc_df = df_grouped.pivot(index=['u','v' ,'cluster'], columns=\"cc\",values = \"vol\")\n",
    "\n",
    "        counts_cc_df = counts_cc_df.fillna(0)\n",
    "        assert counts_cc_df[1].sum() == len(train_labels_df[train_labels_df[\"cc\"] == 1])\n",
    "        assert counts_cc_df[2].sum() == len(train_labels_df[train_labels_df[\"cc\"] == 2])\n",
    "        assert counts_cc_df[3].sum() == len(train_labels_df[train_labels_df[\"cc\"] == 3])\n",
    "        counts_cc_df[\"total\"] = counts_cc_df[1] + counts_cc_df[2] + counts_cc_df[3]\n",
    "        counts_cc_df[\"logit_green\"] = counts_cc_df[1] / counts_cc_df[\"total\"]\n",
    "        counts_cc_df[\"logit_yellow\"] = counts_cc_df[2] / counts_cc_df[\"total\"]\n",
    "        counts_cc_df[\"logit_red\"] = counts_cc_df[3] / counts_cc_df[\"total\"]\n",
    "        del counts_cc_df[1]\n",
    "        del counts_cc_df[2]\n",
    "        del counts_cc_df[3]\n",
    "\n",
    "        counts_cc_df = counts_cc_df.reset_index()\n",
    "        print(f'Median ETAs: {len(counts_cc_df)}')\n",
    "        snapshot_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "        counts_cc_df.to_parquet(snapshot_file, compression='snappy')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in [\"london\",\"madrid\",\"melbourne\"]:\n",
    "    create_counts_cc(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_train_cluster(city):\n",
    "    for train_input_file in sorted((BASEDIR / 'train' / city / 'input').glob('counters_*.parquet')):\n",
    "        train_input = pandas.read_parquet(train_input_file)\n",
    "        train_input['vol'] = np.array(train_input['volumes_1h'].to_numpy().tolist()).sum(axis=1)\n",
    "        cluster_dates_df = get_cluster_ids(city_volume_clusters[city], train_input, ['day', 't'])\n",
    "\n",
    "        train_input = train_input.merge(cluster_dates_df, on=['day', 't'])\n",
    "        output_parquet = BASEDIR / 'train' / city / 'cluster_input'\n",
    "        output_parquet.mkdir(exist_ok=True, parents=True)\n",
    "        train_input = train_input[['node_id','day','t','volumes_1h','cluster']]\n",
    "        train_input.to_parquet(output_parquet / str(train_input_file).split('/')[-1], compression=\"snappy\")\n",
    "def add_test_cluster(city):\n",
    "    for train_input_file in sorted((BASEDIR / 'test' / city / 'input').glob('counters_*.parquet')):\n",
    "        train_input = pandas.read_parquet(train_input_file)\n",
    "        train_input['vol'] = np.array(train_input['volumes_1h'].to_numpy().tolist()).sum(axis=1)\n",
    "        cluster_dates_df = get_cluster_ids(city_volume_clusters[city], train_input, ['test_idx'])\n",
    "\n",
    "        train_input = train_input.merge(cluster_dates_df, on=['test_idx'])\n",
    "        output_parquet = BASEDIR / 'test' / city / 'cluster_input'\n",
    "        output_parquet.mkdir(exist_ok=True, parents=True)\n",
    "        train_input = train_input[['node_id','volumes_1h','test_idx','cluster']]\n",
    "        train_input.to_parquet(output_parquet / str(train_input_file).split('/')[-1], compression=\"snappy\")\n",
    "for city in [\"london\",\"madrid\",\"melbourne\"]:\n",
    "    add_train_cluster(city)\n",
    "    add_test_cluster(city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
